---
layout: post
title: spark ML
---

[Spark ML Programming Guide](http://spark.apache.org/docs/latest/ml-guide.html)


### 核心概念
Spark ML API的目标是为组合多个机器学习算法到一个 pipeline 或 workflow 提供便利。其中管道概念是受
scikit-learn 项目的启发。
- DataFrame: Spark ML使用Spark SQL的 DataFrame 作为数据集
- Transformer: 转化器，是一种将一种 DF 转化为另一种 DF 的算法。一个模型就是一个转换器
- Estimator: 评估者，根据一个 DF，产生一个转化器，比如一种学习算法就是一个estimator，训练DF得到一个模型
- pipeline: 一个管道链接多个 Transformer和Estimator 到一起，组成一个 workflow
- Parameter:

### DataFrame
为了支持多种多样的数据类型，ML 采用了 SQL 中的 DataFrame。
DataFrame 可以从一个 regular RDD 中显式或隐式地创建。

### pipeline 组件：
#### Transformers
一个Transformer实现了一个 transform() 方法，把一个DataFrame 转化为另一种形式。
#### Estimators
Estimator 是训练数据过程中用到的各种机器学习算法、其他算法的抽象。一个 Estimator 实现了一个
fit() 方法，它接收一个DataFrame，训练出一个模型。


``` Scala
// Prepare training data from a list of (id, text, label) tuples.
val training = sqlContext.createDataFrame(Seq(
  (0L, "a b c d e spark", 1.0),
  (1L, "b d", 0.0),
  (2L, "spark f g h", 1.0),
  (3L, "hadoop mapreduce", 0.0),
  (4L, "b spark who", 1.0),
  (5L, "g d a y", 0.0),
  (6L, "spark fly", 1.0),
  (7L, "was mapreduce", 0.0),
  (8L, "e spark program", 1.0),
  (9L, "a e c l", 0.0),
  (10L, "spark compile", 1.0),
  (11L, "hadoop software", 0.0)
)).toDF("id", "text", "label")

// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
val tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words")
val hashingTF = new HashingTF()
  .setInputCol(tokenizer.getOutputCol)
  .setOutputCol("features")
val lr = new LogisticRegression()
  .setMaxIter(10)
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, lr))

// We use a ParamGridBuilder to construct a grid of parameters to search over.
// With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,
// this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.
val paramGrid = new ParamGridBuilder()
  .addGrid(hashingTF.numFeatures, Array(10, 100, 1000))
  .addGrid(lr.regParam, Array(0.1, 0.01))
  .build()

// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.
// This will allow us to jointly choose parameters for all Pipeline stages.
// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.
// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric
// is areaUnderROC.
val cv = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(new BinaryClassificationEvaluator)
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(2) // Use 3+ in practice

// Run cross-validation, and choose the best set of parameters.
val cvModel = cv.fit(training)

// Prepare test documents, which are unlabeled (id, text) tuples.
val test = sqlContext.createDataFrame(Seq(
  (4L, "spark i j k"),
  (5L, "l m n"),
  (6L, "mapreduce spark"),
  (7L, "apache hadoop")
)).toDF("id", "text")

// Make predictions on test documents. cvModel uses the best model found (lrModel).
cvModel.transform(test)
  .select("id", "text", "probability", "prediction")
  .collect()
  .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>
    println(s"($id, $text) --> prob=$prob, prediction=$prediction")
  }

```
